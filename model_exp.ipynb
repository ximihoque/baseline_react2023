{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78dcbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from model import TransformerVAE\n",
    "from utils import AverageMeter\n",
    "from dataset import get_dataloader\n",
    "from model.losses import VAELoss, div_loss\n",
    "from dataset import ReactionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dfba6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b26513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33aca895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num threads:  1\n",
      "Using HuBERT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                   | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker video shape:  torch.Size([2, 1568, 1024])\n",
      "speaker audio shape:  torch.Size([2, 480000])\n",
      "listener 3d mm shape:  torch.Size([2, 750, 58])\n",
      "running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/surbhi/anaconda3/envs/react_new/lib/python3.8/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343962757/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:13<00:00, 13.96s/it][W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import dill as pickle\n",
    "from dataset import ReactionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model import TransformerVAE\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "# from model import HuBERTEncoder\n",
    "\n",
    "\n",
    "print ('num threads: ', torch.get_num_threads())\n",
    "# torch.set_num_threads(1)\n",
    "audio_flag = True\n",
    "model = TransformerVAE(\n",
    "    img_size=256,    \n",
    "    use_hubert=audio_flag,\n",
    "audio_dim=128, # becomes irrelevant in case for hubert\n",
    "max_seq_len=751, \n",
    "seq_len=750)\n",
    "\n",
    "dataset = ReactionDataset('../data', 'data/sample_udiva.csv', clip_length=750, use_raw_audio=audio_flag, load_3dmm_l=True, mode='val')\n",
    "shuffle = True \n",
    "# sample = dataset[0]\n",
    "# hubert = HuBERTEncoder(1024)\n",
    "# hubert = hubert.cuda()\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=2, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "checkpoints = torch.load('./results/marlin_hubert/best_checkpoint.pth', map_location=torch.device('cpu'))\n",
    "state_dict = checkpoints['state_dict']\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "# speaker_video_clip, speaker_audio_clip, _, _, _, _, listener_emotion, listener_3dmm, listener_references = dataset[0]\n",
    "# print (listener_3dmm.shape)\n",
    "# print (\"enumeratin\")\n",
    "for batch_idx, (speaker_video_clip, \n",
    "                speaker_video_orig, \n",
    "                speaker_audio_clip, speaker_emotion, _, \n",
    "                listener_video_clip, _, listener_emotion, listener_3dmm, listener_references) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "    print ('speaker video shape: ', speaker_video_clip.shape)\n",
    "    print ('speaker audio shape: ', speaker_audio_clip.shape)\n",
    "    print ('listener 3d mm shape: ', listener_3dmm.shape)\n",
    "    print (\"running inference...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        listener_3dmm_out, listener_emotion_out, distribution = model(speaker_video_clip.cuda(), speaker_audio_clip.cuda())\n",
    "\n",
    "print (\"done.\")\n",
    "# print ('Batch idx: ', batch_idx)\n",
    "# train_loader = get_dataloader(args, \"data/train.csv\", load_ref=False, load_video_l=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aac9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# render = Render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7d9460c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0571,  0.1654,  0.0706,  ...,  0.0467, -0.0047,  0.0558],\n",
       "        [-0.0571,  0.1654,  0.0706,  ...,  0.0467, -0.0047,  0.0558],\n",
       "        [-0.0571,  0.1654,  0.0706,  ...,  0.0467, -0.0047,  0.0558],\n",
       "        ...,\n",
       "        [-0.0571,  0.1654,  0.0706,  ...,  0.0467, -0.0047,  0.0558],\n",
       "        [-0.0571,  0.1654,  0.0706,  ...,  0.0467, -0.0047,  0.0558],\n",
       "        [-0.0571,  0.1654,  0.0706,  ...,  0.0467, -0.0047,  0.0558]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d36dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee1d29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "react_new",
   "language": "python",
   "name": "react_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
