{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78dcbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from model import TransformerVAE\n",
    "from utils import AverageMeter\n",
    "from dataset import get_dataloader\n",
    "from model.losses import VAELoss, div_loss\n",
    "from dataset import ReactionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dfba6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b26513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33aca895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num threads:  1\n",
      "Using HuBERT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                                   | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker video shape:  torch.Size([1, 23, 1024])\n",
      "speaker audio shape:  torch.Size([1, 480000])\n",
      "listener 3d mm shape:  torch.Size([1, 750, 58])\n",
      "running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.24s/it][W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import dill as pickle\n",
    "from dataset import ReactionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model import TransformerVAE\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "# from model import HuBERTEncoder\n",
    "\n",
    "\n",
    "print ('num threads: ', torch.get_num_threads())\n",
    "# torch.set_num_threads(1)\n",
    "audio_flag = True\n",
    "model = TransformerVAE(\n",
    "    img_size=256,    \n",
    "    use_hubert=audio_flag,\n",
    "audio_dim=128, # becomes irrelevant in case for hubert\n",
    "max_seq_len=751, \n",
    "seq_len=750)\n",
    "\n",
    "dataset = ReactionDataset('../data', 'data/sample_udiva.csv', clip_length=750, use_raw_audio=audio_flag, load_3dmm_l=True, mode='val')\n",
    "shuffle = True \n",
    "# sample = dataset[0]\n",
    "# hubert = HuBERTEncoder(1024)\n",
    "# hubert = hubert.cuda()\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=2, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "checkpoints = torch.load('./results/marlin_hubert_27jun/cur_checkpoint.pth', map_location=torch.device('cpu'))\n",
    "state_dict = checkpoints['state_dict']\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.cuda()\n",
    "# model.eval()\n",
    "# speaker_video_clip, speaker_audio_clip, _, _, _, _, listener_emotion, listener_3dmm, listener_references = dataset[0]\n",
    "# print (listener_3dmm.shape)\n",
    "# print (\"enumeratin\")\n",
    "for batch_idx, (speaker_video_clip, \n",
    "                speaker_video_orig, \n",
    "                speaker_audio_clip, speaker_emotion, _, \n",
    "                listener_video_clip, _, listener_emotion, listener_3dmm, listener_references) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "    print ('speaker video shape: ', speaker_video_clip.shape)\n",
    "    print ('speaker audio shape: ', speaker_audio_clip.shape)\n",
    "    print ('listener 3d mm shape: ', listener_3dmm.shape)\n",
    "    print (\"running inference...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        listener_3dmm_out, listener_emotion_out, distribution = model(speaker_video_clip.cuda(), speaker_audio_clip.cuda())\n",
    "\n",
    "print (\"done.\")\n",
    "# print ('Batch idx: ', batch_idx)\n",
    "# train_loader = get_dataloader(args, \"data/train.csv\", load_ref=False, load_video_l=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aac9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.losses import MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7d9460c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_loss(listener_3dmm_out, listener_3dmm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ee1d29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([750, 58])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listener_3dmm_out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e5fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2832d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/train_neg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8373dc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          UDIVA/talk/023102/FC1/8\n",
       "1          UDIVA/talk/023102/FC1/9\n",
       "2         UDIVA/talk/023102/FC1/10\n",
       "3         UDIVA/talk/023102/FC1/14\n",
       "4         UDIVA/talk/023191/FC1/19\n",
       "                   ...            \n",
       "2629     UDIVA/animal/140170/FC1/7\n",
       "2630    UDIVA/animal/140170/FC1/10\n",
       "2631    UDIVA/animal/140170/FC1/11\n",
       "2632    UDIVA/animal/140170/FC1/19\n",
       "2633    UDIVA/animal/140170/FC1/20\n",
       "Name: speaker_path, Length: 2634, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['speaker_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26c641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlin",
   "language": "python",
   "name": "marlin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
