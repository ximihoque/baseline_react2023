{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161b4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from model import TransformerVAEBaseline\n",
    "from model import TransformerVAE\n",
    "from model import TransformerVAEHubert\n",
    "from utils import AverageMeter\n",
    "from dataset import get_dataloader\n",
    "from model.losses import VAELoss, div_loss\n",
    "from dataset import ReactionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61d05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bf08587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cded079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerVAEBaseline(\n",
    "#     online=False,\n",
    "#     img_size=256,    \n",
    "# #     use_hubert=audio_flag,\n",
    "#     audio_dim=78, # becomes irrelevant in case for hubert\n",
    "# #     max_seq_len=751, \n",
    "#     seq_len=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8162e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints = torch.load('../data/offline_TransVAE.pth', map_location=torch.device('cpu'))\n",
    "# state_dict = checkpoints['state_dict']\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "262761e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using HuBERT.\n"
     ]
    }
   ],
   "source": [
    "model = TransformerVAEHubert(\n",
    "    online=False,\n",
    "    img_size=256,    \n",
    "    use_hubert=True,\n",
    "    audio_dim=78, # becomes irrelevant in case for hubert\n",
    "    max_seq_len=751, \n",
    "    seq_len=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349cde28",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = torch.load('./results/hubert_27jun/cur_checkpoint.pth', map_location=torch.device('cpu'))\n",
    "state_dict = checkpoints['state_dict']\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c772a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TransformerVAE(\n",
    "#     img_size=256,    \n",
    "#     use_hubert=True,\n",
    "# audio_dim=128, # becomes irrelevant in case for hubert\n",
    "# max_seq_len=751, \n",
    "# seq_len=750)\n",
    "# checkpoints = torch.load('./results/marlin_hubert_25jun/best_checkpoint.pth', map_location=torch.device('cpu'))\n",
    "# state_dict = checkpoints['state_dict']\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "079eb7b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num threads:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                | 0/1 [00:00<?, ?it/s]/home/surbhi/anaconda3/envs/marlin/lib/python3.11/site-packages/torch/nn/modules/activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343970094/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker video shape:  torch.Size([1, 23, 1024])\n",
      "speaker audio shape:  torch.Size([1, 480000])\n",
      "listener 3d mm shape:  torch.Size([1, 750, 58])\n",
      "running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W CudaIPCTypes.cpp:15] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:09<00:00,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import dill as pickle\n",
    "from dataset import ReactionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model import TransformerVAE\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "# from model import HuBERTEncoder\n",
    "audio_flag = True\n",
    "\n",
    "print ('num threads: ', torch.get_num_threads())\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "# model = TransformerVAE(\n",
    "#     img_size=256,    \n",
    "#     use_hubert=audio_flag,\n",
    "# audio_dim=128, # becomes irrelevant in case for hubert\n",
    "# max_seq_len=751, \n",
    "# seq_len=750)\n",
    "\n",
    "dataset = ReactionDataset('../data', 'data/sample_udiva.csv', clip_length=750, use_raw_audio=audio_flag, load_3dmm_l=True, load_emotion_l=True, mode='val')\n",
    "shuffle = True \n",
    "# sample = dataset[0]\n",
    "# hubert = HuBERTEncoder(1024)\n",
    "# hubert = hubert.cuda()\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "\n",
    "# model = model.cuda()\n",
    "model.eval()\n",
    "# speaker_video_clip, speaker_audio_clip, _, _, _, _, listener_emotion, listener_3dmm, listener_references = dataset[0]\n",
    "# print (listener_3dmm.shape)\n",
    "# print (\"enumeratin\")\n",
    "for batch_idx, (speaker_video_clip, \n",
    "                speaker_video_orig, \n",
    "                speaker_audio_clip, speaker_emotion, _, \n",
    "                listener_video_clip, _, listener_emotion, listener_3dmm, listener_references) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "    print ('speaker video shape: ', speaker_video_clip.shape)\n",
    "    print ('speaker audio shape: ', speaker_audio_clip.shape)\n",
    "    print ('listener 3d mm shape: ', listener_3dmm.shape)\n",
    "    print (\"running inference...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        listener_3dmm_out, listener_emotion_out, distribution = model(speaker_video_clip.cuda(), speaker_audio_clip.cuda())\n",
    "\n",
    "print (\"done.\")\n",
    "# print ('Batch idx: ', batch_idx)\n",
    "# train_loader = get_dataloader(args, \"data/train.csv\", load_ref=False, load_video_l=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbd94b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3035, 0.4337, 0.2760,  ..., 0.2348, 0.3445, 0.1702],\n",
       "         [0.3035, 0.4337, 0.2760,  ..., 0.2348, 0.3445, 0.1702],\n",
       "         [0.3035, 0.4337, 0.2760,  ..., 0.2348, 0.3445, 0.1702],\n",
       "         ...,\n",
       "         [0.3035, 0.4337, 0.2760,  ..., 0.2348, 0.3445, 0.1702],\n",
       "         [0.3035, 0.4337, 0.2760,  ..., 0.2348, 0.3445, 0.1702],\n",
       "         [0.3035, 0.4337, 0.2760,  ..., 0.2348, 0.3445, 0.1702]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listener_emotion_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88faac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.speaker_behaviour_encoder(speaker_video_clip.cuda(), speaker_audio_clip.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f76b8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0484, 0.0000, 0.0000,  ..., 0.0000, 1.3758, 0.0000],\n",
       "         [0.1532, 0.0000, 0.0000,  ..., 0.0000, 1.5627, 0.0000],\n",
       "         [0.2996, 0.0000, 0.0000,  ..., 0.0000, 1.8209, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.6367, 0.6167,  ..., 0.4307, 0.0000, 0.6856],\n",
       "         [0.0000, 0.6367, 0.6167,  ..., 0.4642, 0.0000, 0.6856],\n",
       "         [0.0000, 0.5422, 0.5230,  ..., 0.4922, 0.0000, 0.5608]]],\n",
       "       device='cuda:0', grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.speaker_behaviour_encoder(speaker_video_clip.cuda(), speaker_audio_clip.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "361cec48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0025, -0.0115, -0.4267,  ..., -0.3654,  0.0414, -0.3364],\n",
       "         [ 0.0021, -0.0116, -0.4265,  ..., -0.3645,  0.0414, -0.3358],\n",
       "         [ 0.0029, -0.0121, -0.4264,  ..., -0.3634,  0.0427, -0.3365],\n",
       "         ...,\n",
       "         [ 0.0118, -0.0681, -0.3716,  ..., -0.3249,  0.1647, -0.2707],\n",
       "         [ 0.0114, -0.0670, -0.3699,  ..., -0.3214,  0.1687, -0.2681],\n",
       "         [ 0.0035, -0.0682, -0.3893,  ..., -0.3670,  0.1548, -0.2583]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b256465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_encoder = model.speaker_behaviour_encoder.video_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bc04f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoEncoder(\n",
       "  (Conv3D): ConvBlock(\n",
       "    (conv1): Conv3d(3, 32, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
       "    (in1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (maxpool): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 0, 0), dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv3d(32, 128, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
       "    (in2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv3): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "    (in3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (conv4): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "    (in4): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (conv5): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "    (in5): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=128, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "095e3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(video_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "005a6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted.save('video_encoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8786b9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=VideoEncoder\n",
       "  (Conv3D): RecursiveScriptModule(\n",
       "    original_name=ConvBlock\n",
       "    (conv1): RecursiveScriptModule(original_name=Conv3d)\n",
       "    (in1): RecursiveScriptModule(original_name=InstanceNorm3d)\n",
       "    (maxpool): RecursiveScriptModule(original_name=MaxPool3d)\n",
       "    (conv2): RecursiveScriptModule(original_name=Conv3d)\n",
       "    (in2): RecursiveScriptModule(original_name=InstanceNorm3d)\n",
       "    (relu): RecursiveScriptModule(original_name=ReLU)\n",
       "    (conv3): RecursiveScriptModule(original_name=Conv3d)\n",
       "    (in3): RecursiveScriptModule(original_name=InstanceNorm3d)\n",
       "    (conv4): RecursiveScriptModule(original_name=Conv3d)\n",
       "    (in4): RecursiveScriptModule(original_name=InstanceNorm3d)\n",
       "    (conv5): RecursiveScriptModule(original_name=Conv3d)\n",
       "    (in5): RecursiveScriptModule(original_name=InstanceNorm3d)\n",
       "  )\n",
       "  (fc): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.jit.load('video_encoder.pt')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "801f57a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0025, -0.0115, -0.4267,  ..., -0.3654,  0.0414, -0.3364],\n",
       "         [ 0.0021, -0.0116, -0.4265,  ..., -0.3645,  0.0414, -0.3358],\n",
       "         [ 0.0029, -0.0121, -0.4264,  ..., -0.3634,  0.0427, -0.3365],\n",
       "         ...,\n",
       "         [ 0.0118, -0.0681, -0.3716,  ..., -0.3249,  0.1647, -0.2707],\n",
       "         [ 0.0114, -0.0670, -0.3699,  ..., -0.3214,  0.1687, -0.2681],\n",
       "         [ 0.0035, -0.0682, -0.3893,  ..., -0.3670,  0.1548, -0.2583]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(speaker_video_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefde72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlin",
   "language": "python",
   "name": "marlin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
