{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161b4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from model import TransformerVAEBaseline\n",
    "from utils import AverageMeter\n",
    "from dataset import get_dataloader\n",
    "from model.losses import VAELoss, div_loss\n",
    "from dataset import ReactionDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61d05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf08587",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "context has already been set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mmultiprocessing\u001b[38;5;241m.\u001b[39mset_start_method(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/marlin/lib/python3.11/multiprocessing/context.py:247\u001b[0m, in \u001b[0;36mDefaultContext.set_start_method\u001b[0;34m(self, method, force)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_start_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actual_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force:\n\u001b[0;32m--> 247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext has already been set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m force:\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actual_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: context has already been set"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "079eb7b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num threads:  1\n",
      "Using HuBERT.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TransformerVAE:\n\tMissing key(s) in state_dict: \"speaker_behaviour_encoder.fusion_layer.0.weight\", \"speaker_behaviour_encoder.fusion_layer.0.bias\", \"speaker_behaviour_encoder.fusion_layer.3.weight\", \"speaker_behaviour_encoder.fusion_layer.3.bias\", \"speaker_behaviour_encoder.fusion_layer.6.weight\", \"speaker_behaviour_encoder.fusion_layer.6.bias\". \n\tUnexpected key(s) in state_dict: \"speaker_behaviour_encoder.video_encoder.conv1d.weight\", \"speaker_behaviour_encoder.video_encoder.conv1d.bias\", \"speaker_behaviour_encoder.audio_feature_map.weight\", \"speaker_behaviour_encoder.audio_feature_map.bias\", \"speaker_behaviour_encoder.video_feature_map.weight\", \"speaker_behaviour_encoder.video_feature_map.bias\", \"speaker_behaviour_encoder.fusion_layer.weight\", \"speaker_behaviour_encoder.fusion_layer.bias\". \n\tsize mismatch for speaker_behaviour_encoder.audio_encoder.lin.weight: copying a param with shape torch.Size([128, 746]) from checkpoint, the shape in current model is torch.Size([1024, 1022]).\n\tsize mismatch for speaker_behaviour_encoder.audio_encoder.lin.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m checkpoints \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/marlin_hubert_25jun/best_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     29\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m checkpoints[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/marlin/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TransformerVAE:\n\tMissing key(s) in state_dict: \"speaker_behaviour_encoder.fusion_layer.0.weight\", \"speaker_behaviour_encoder.fusion_layer.0.bias\", \"speaker_behaviour_encoder.fusion_layer.3.weight\", \"speaker_behaviour_encoder.fusion_layer.3.bias\", \"speaker_behaviour_encoder.fusion_layer.6.weight\", \"speaker_behaviour_encoder.fusion_layer.6.bias\". \n\tUnexpected key(s) in state_dict: \"speaker_behaviour_encoder.video_encoder.conv1d.weight\", \"speaker_behaviour_encoder.video_encoder.conv1d.bias\", \"speaker_behaviour_encoder.audio_feature_map.weight\", \"speaker_behaviour_encoder.audio_feature_map.bias\", \"speaker_behaviour_encoder.video_feature_map.weight\", \"speaker_behaviour_encoder.video_feature_map.bias\", \"speaker_behaviour_encoder.fusion_layer.weight\", \"speaker_behaviour_encoder.fusion_layer.bias\". \n\tsize mismatch for speaker_behaviour_encoder.audio_encoder.lin.weight: copying a param with shape torch.Size([128, 746]) from checkpoint, the shape in current model is torch.Size([1024, 1022]).\n\tsize mismatch for speaker_behaviour_encoder.audio_encoder.lin.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([1024])."
     ]
    }
   ],
   "source": [
    "# import dill as pickle\n",
    "from dataset import ReactionDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from model import TransformerVAE\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "# from model import HuBERTEncoder\n",
    "\n",
    "\n",
    "print ('num threads: ', torch.get_num_threads())\n",
    "# torch.set_num_threads(1)\n",
    "audio_flag = True\n",
    "model = TransformerVAE(\n",
    "    img_size=256,    \n",
    "    use_hubert=audio_flag,\n",
    "audio_dim=128, # becomes irrelevant in case for hubert\n",
    "max_seq_len=751, \n",
    "seq_len=750)\n",
    "\n",
    "dataset = ReactionDataset('../data', 'data/sample_udiva.csv', clip_length=750, use_raw_audio=audio_flag, load_3dmm_l=True)\n",
    "shuffle = True \n",
    "# sample = dataset[0]\n",
    "# hubert = HuBERTEncoder(1024)\n",
    "# hubert = hubert.cuda()\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=1, shuffle=shuffle, num_workers=2)\n",
    "\n",
    "checkpoints = torch.load('./results/marlin_hubert_25jun/best_checkpoint.pth', map_location=torch.device('cpu'))\n",
    "state_dict = checkpoints['state_dict']\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "# speaker_video_clip, speaker_audio_clip, _, _, _, _, listener_emotion, listener_3dmm, listener_references = dataset[0]\n",
    "# print (listener_3dmm.shape)\n",
    "# print (\"enumeratin\")\n",
    "for batch_idx, (speaker_video_clip, \n",
    "                speaker_video_orig, \n",
    "                speaker_audio_clip, speaker_emotion, _, \n",
    "                listener_video_clip, _, listener_emotion, listener_3dmm, listener_references) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "    print ('speaker video shape: ', speaker_video_clip.shape)\n",
    "    print ('speaker audio shape: ', speaker_audio_clip.shape)\n",
    "    print ('listener 3d mm shape: ', listener_3dmm.shape)\n",
    "    print (\"running inference...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        listener_3dmm_out, listener_emotion_out, distribution = model(speaker_video_clip.cuda(), speaker_audio_clip.cuda())\n",
    "\n",
    "print (\"done.\")\n",
    "# print ('Batch idx: ', batch_idx)\n",
    "# train_loader = get_dataloader(args, \"data/train.csv\", load_ref=False, load_video_l=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256465e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76b8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlin",
   "language": "python",
   "name": "marlin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
